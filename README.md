# VisionPulse - AI Video Creation System

Transform written scripts into fully generated short videos using AI imagery and narration powered by OpenAI.

## Features

- ðŸŽ¬ **Automated Video Generation**: From script to video in one click
- ðŸŽ¨ **Multiple Visual Styles**: Choose from preset artistic styles
- ðŸŽ™ï¸ **AI Narration**: Multiple voice options using OpenAI TTS
- ðŸ–¼ï¸ **AI Image Generation**: DALL-E 3 powered visuals
- ðŸŽ¥ **Cinematic Motion Effects**: Ken Burns zoom, pan effects for dynamic feel
- ðŸ“Š **Clean Dashboard**: Simple Next.js interface with video gallery
- âš¡ **LangGraph Workflow**: Sequential pipeline for reliable generation

> **Note on OpenAI Sora**: Currently using DALL-E 3 images with cinematic motion effects. When OpenAI Sora becomes publicly available via API, the system is designed to easily integrate native video generation for even more dynamic results.

## Tech Stack

- **Backend**: Python, LangGraph, FastAPI, OpenAI API
- **Frontend**: Next.js 14, React, TailwindCSS
- **Video Processing**: MoviePy
- **Database**: SQLite with SQLAlchemy

## Quick Start

### 1. Set Up Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate it
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your OpenAI API key
```

### 3. Run Backend

```bash
# Start FastAPI server
python -m uvicorn backend.main:app --reload
```

API will be available at: http://localhost:8000

### 4. Run Frontend

```bash
cd frontend
npm install
npm run dev
```

Dashboard will be available at: http://localhost:3000

## Project Structure

```
VisionPulse/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/                 # FastAPI routes
â”‚   â”œâ”€â”€ workflows/           # LangGraph workflows
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â””â”€â”€ config/              # Configuration files
â”œâ”€â”€ frontend/                # Next.js application
â”œâ”€â”€ output/                  # Generated files
â”‚   â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ audio/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

## Usage

1. Open the dashboard at http://localhost:3000
2. Click "Create New Video"
3. Paste your script
4. Choose a visual style
5. Select a narration voice
6. Add keywords (optional)
7. Click "Generate Video"
8. Download your finished video!

## API Endpoints

- `POST /api/videos/create` - Create a new video
- `GET /api/videos` - List all videos
- `GET /api/videos/{video_id}` - Get video details
- `GET /api/styles` - List available styles
- `GET /api/voices` - List available voices

## License

MIT
